---
layout: single
title: "AI tech 네번째 날!"
---
# 네번째 날!

오늘은 많이 바빴습니다

수업도 듣는 것도 조금 밀렸고 과제도 못해서 부리나케 했거든요

그래도 그만큼 배운 것도 많죠

과제중에 선형 회귀 문제가 있었는데 이전까지는 코드를 따라쓰는 것에 그쳤다면

이제는 각 코드가 실행되면서 어떻게 진행되는지 머리로 이해할 수준까지는 되었습니다.

그렇다고 아직 바닥부터 모델을 구현하는건 한참먼것 같습니다...

<pre>
  <code>
    import numpy as np

    num_epochs = 2000 # epoch를 2000으로 설정
    model.train() # 학습을 위해 모델이 gradient를 저장하도록 설정
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        data_num = 0
        for inputs, targets in train_loader: # 각 배치마다 반복
            optimizer.zero_grad() # 옵티마이저의 gradient 초기화
            outputs = model(inputs) # 데이터를 넣었을 때의 모델의 출력값 저장
            loss = criterion(outputs, targets) # MSE 손실 계산
            loss.backward() # gradient descent 수행
            optimizer.step() # SGD 방식의 최적화 진행
            epoch_loss += loss.item()*inputs.shape[0]
            data_num += inputs.shape[0]
        if (epoch+1) % 100 == 0: # 100번의 epoch마다 학습 데이터의 손실함수 출력
            print(f"Epoch {epoch+1}, Loss(RMSE): {np.sqrt(epoch_loss/data_num)}")
  </code>
</pre>

코드 중 일부만 가져왔습니다.

이 중에 한창 고민했던것은 loss.backward()를 통해 계산을 하고 나면 그 결과를 어떻게 optimizer가 알고 가중치를 업데이트하나 의문이 들었습니다.

lose.backward()나 optimizer.step()에서도 어떤 파라미터가 넘어간거 같지는 않았거든요.

여기저기 검색해본 결과 opimizer 에서 step() 이 실행되면 알아서 손실함수가 계산한 값을 알아서 가져와서 가중치를 업데이트 한다는 것을 알았습니다.

이렇듯 뭔가 조금씩 알아가는게 재밌는 것 같습니다 :)
